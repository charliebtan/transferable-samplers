# this does cosine annealing from max_lr to max_lr / div_factor / final_div_factor
# if pct_start != 0.0 you warmup from max_lr / div_factor to max_lr too
# normally you use this per-step but I've just got it per-epoch
_target_: torch.optim.lr_scheduler.OneCycleLR
_partial_: true
max_lr: ${model.optimizer.lr}
total_steps: ${trainer.max_epochs}
pct_start: 0.05
anneal_strategy: cos
cycle_momentum: false
div_factor: 5e2
final_div_factor: 1.0
